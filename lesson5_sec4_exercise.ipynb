{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"lesson5_sec4_exercise.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ZOgS7qdTj9PS"},"source":["# 画像からキャプションを生成してみよう"]},{"cell_type":"markdown","metadata":{"id":"9FVhoPRaj9Pt"},"source":["## 目次\n","- Section 4 実装②\n","    - CNN-LSTM with Attention"]},{"cell_type":"markdown","metadata":{"id":"cwN8L_YKj9Px"},"source":["## Section 4 実装②"]},{"cell_type":"markdown","metadata":{"id":"EQKiyczTj9Py"},"source":["ここでは、Section 3で学んだ知識を使って、AttentionつきのCNN-LSTMを実装し、改めてMS-COCOデータセットでキャプション生成を行いましょう。"]},{"cell_type":"markdown","metadata":{"id":"2h2b07t7j9P0"},"source":["### 4.0 データの準備"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a-t1qKKRkgXf","executionInfo":{"status":"ok","timestamp":1620520885184,"user_tz":-540,"elapsed":20767,"user":{"displayName":"Akihiro Roppongi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghn5ogbwFTFNltWRIA-1PDOiPxtOW_89Kxmms08iWk=s64","userId":"12484382358284301373"}},"outputId":"2be43eea-5fbc-46fd-ff6c-9ae9c2b401b7"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GftfbjYfj9P1","executionInfo":{"status":"ok","timestamp":1620521069482,"user_tz":-540,"elapsed":178729,"user":{"displayName":"Akihiro Roppongi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghn5ogbwFTFNltWRIA-1PDOiPxtOW_89Kxmms08iWk=s64","userId":"12484382358284301373"}}},"source":["import numpy as np\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","def load_data(file_path, tokenizer = None):\n","    whole_texts = []\n","    for line in open(file_path, encoding='utf-8'):\n","        whole_texts.append(\"<s> \" + line.strip() + \" </s>\")\n","        \n","    if tokenizer == None :\n","        tokenizer = Tokenizer(filters=\"\")\n","        tokenizer.fit_on_texts(whole_texts)\n","    \n","    return tokenizer.texts_to_sequences(whole_texts), tokenizer\n","\n","x_train = np.load('/content/gdrive/MyDrive/dl4us/dl4us-master/lesson5/data/x_train.npy')\n","x_valid = np.load('/content/gdrive/MyDrive/dl4us/dl4us-master/lesson5/data/x_valid.npy')\n","\n","# 読み込み＆Tokenizerによる数値化\n","y_train, tokenizer_train = load_data('/content/gdrive/MyDrive/dl4us/dl4us-master/lesson5/data/y_train.txt')\n","y_valid, _ = load_data('/content/gdrive/MyDrive/dl4us/dl4us-master/lesson5/data/y_valid.txt', tokenizer_train)\n","\n","vocab_size = len(tokenizer_train.word_index) + 1\n","\n","# パディング\n","y_train = pad_sequences(y_train, padding='post')\n","y_valid = pad_sequences(y_valid, padding='post')\n","\n","caption_len = len(y_train[0])"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gTuo-mm4j9P4"},"source":["### 4.1 モデル構築"]},{"cell_type":"markdown","metadata":{"id":"1SeHOX98j9P6"},"source":["#### Encoder"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lADyYXPjj9P7","executionInfo":{"status":"ok","timestamp":1620521083836,"user_tz":-540,"elapsed":7223,"user":{"displayName":"Akihiro Roppongi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghn5ogbwFTFNltWRIA-1PDOiPxtOW_89Kxmms08iWk=s64","userId":"12484382358284301373"}},"outputId":"657a5923-19ea-4ee8-c403-27c11195dc64"},"source":["from tensorflow.keras import backend as K\n","from tensorflow.keras.layers import Input, Flatten, Lambda\n","from tensorflow.keras.applications.vgg16 import VGG16\n","\n","K.clear_session()\n","\n","encoder_input = Input(shape=(224, 224, 3))\n","encoder_input_normalized = Lambda(lambda x: x / 255.)(encoder_input) # [0, 255) -> [0, 1)\n","encoder = VGG16(weights='imagenet', include_top=False, input_tensor=encoder_input_normalized)\n","\n","# パラメータを固定\n","for layer in encoder.layers:\n","    layer.trainable = False\n","\n","# CNNの出力\n","u = Flatten()(encoder.output)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58892288/58889256 [==============================] - 1s 0us/step\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QPPK1Aaoj9P9"},"source":["#### Decoder"]},{"cell_type":"code","metadata":{"id":"jJKooasUj9P-","executionInfo":{"status":"ok","timestamp":1620521084924,"user_tz":-540,"elapsed":7476,"user":{"displayName":"Akihiro Roppongi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghn5ogbwFTFNltWRIA-1PDOiPxtOW_89Kxmms08iWk=s64","userId":"12484382358284301373"}}},"source":["from tensorflow.keras.layers import Dense, Embedding, LSTM\n","\n","emb_dim = 128\n","hid_dim = 128\n","\n","# LSTMの初期状態\n","decoded_states = [Dense(hid_dim)(u), Dense(hid_dim)(u)] # h_0、c_0 に対応\n","\n","# 層の定義\n","decoder_input = Input(shape=(caption_len,))\n","embedding = Embedding(vocab_size, emb_dim, mask_zero=True)\n","lstm = LSTM(hid_dim, activation='tanh', return_sequences=True, return_state=True)\n","\n","# 層の接続\n","decoder_embedded = embedding(decoder_input)\n","decoder_output, _, _ = lstm(decoder_embedded, initial_state=decoded_states) # 第2、3戻り値(最終ステップのh、c)は無視"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jECENgESj9QA"},"source":["#### Attention"]},{"cell_type":"code","metadata":{"id":"oet3gU7yj9QB","executionInfo":{"status":"ok","timestamp":1620521084927,"user_tz":-540,"elapsed":5891,"user":{"displayName":"Akihiro Roppongi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghn5ogbwFTFNltWRIA-1PDOiPxtOW_89Kxmms08iWk=s64","userId":"12484382358284301373"}}},"source":["from tensorflow.keras.layers import Reshape, Activation, concatenate, dot\n","\n","# 0. reshape: (7, 7, 512) -> (49, 512)\n","u_map = Reshape((7*7, 512))(u)\n","\n","# 1. スコアの計算\n","dense_att = Dense(hid_dim)\n","score = dot([decoder_output, dense_att(u_map)], axes=-1)\n","\n","# 2. 重みの計算\n","attention = Activation('softmax')(score)\n","\n","# 3. 文脈ベクトルの計算\n","context = dot([attention, u_map], axes=(2,1))\n","\n","# 4. 出力ベクトルの計算\n","attention_dense = Dense(hid_dim, activation='tanh')\n","output_dense = Dense(vocab_size, activation='softmax')\n","concat = concatenate([context, decoder_output], axis=2)\n","attentional = attention_dense(concat)\n","y_pred = output_dense(attentional)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q29igBkOj9QC"},"source":["#### model化"]},{"cell_type":"code","metadata":{"id":"xGQSYX4sj9QD","executionInfo":{"status":"ok","timestamp":1620521088897,"user_tz":-540,"elapsed":658,"user":{"displayName":"Akihiro Roppongi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghn5ogbwFTFNltWRIA-1PDOiPxtOW_89Kxmms08iWk=s64","userId":"12484382358284301373"}}},"source":["from tensorflow.keras.models import Model\n","\n","model = Model([encoder_input, decoder_input], y_pred)\n","model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop')"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LpSkE3m6j9QF"},"source":["### 2.2 モデルの学習"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y0tsNa1Jj9QF","executionInfo":{"status":"ok","timestamp":1620522225493,"user_tz":-540,"elapsed":1128172,"user":{"displayName":"Akihiro Roppongi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghn5ogbwFTFNltWRIA-1PDOiPxtOW_89Kxmms08iWk=s64","userId":"12484382358284301373"}},"outputId":"c463c195-6067-493b-bd8a-9b6abe0301b5"},"source":["import numpy as np\n","\n","train_target = np.hstack((y_train[:, 1:], np.zeros((len(y_train),1), dtype=np.int32)))\n","\n","model.fit([x_train, y_train], np.expand_dims(train_target, -1), batch_size=64, epochs=5, verbose=1, validation_split=0.2)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","625/625 [==============================] - 263s 348ms/step - loss: 1.1881 - val_loss: 0.8987\n","Epoch 2/5\n","625/625 [==============================] - 216s 346ms/step - loss: 0.8197 - val_loss: 0.8212\n","Epoch 3/5\n","625/625 [==============================] - 216s 346ms/step - loss: 0.7462 - val_loss: 0.7894\n","Epoch 4/5\n","625/625 [==============================] - 216s 346ms/step - loss: 0.7111 - val_loss: 0.7738\n","Epoch 5/5\n","625/625 [==============================] - 216s 346ms/step - loss: 0.6863 - val_loss: 0.7624\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f966d882c90>"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"nHp4akmKj9QH"},"source":["### 2.3 モデルによる生成"]},{"cell_type":"code","metadata":{"id":"Mu5GMCHDj9QH","executionInfo":{"status":"ok","timestamp":1620522229958,"user_tz":-540,"elapsed":1643,"user":{"displayName":"Akihiro Roppongi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghn5ogbwFTFNltWRIA-1PDOiPxtOW_89Kxmms08iWk=s64","userId":"12484382358284301373"}}},"source":["encoder_model = Model(encoder_input, [u_map]+decoded_states)\n","\n","decoder_states_inputs = [Input(shape=(hid_dim,)), Input(shape=(hid_dim,))]\n","\n","decoder_input = Input(shape=(1,))\n","decoder_embeded = embedding(decoder_input)\n","decoded_seq, *decoder_states = lstm(decoder_embeded, initial_state=decoder_states_inputs)\n","\n","decoder_model = Model([decoder_input] + decoder_states_inputs, [decoded_seq] + decoder_states)\n","\n","# Attention\n","u_map_in, decoded_seq_in = Input(shape=(7*7, 512)), Input(shape=(1, hid_dim))\n","score = dot([decoded_seq_in, dense_att(u_map_in)], axes=-1)\n","attention = Activation('softmax')(score)\n","context = dot([attention, u_map_in], axes=(2,1))\n","concat = concatenate([context, decoded_seq_in], axis=2)\n","attentional = attention_dense(concat)\n","attention_outputs = output_dense(attentional)\n","\n","attention_model = Model([u_map_in, decoded_seq_in], [attention_outputs, attention])"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YaG6-NrVj9QK"},"source":["#### Beam Search"]},{"cell_type":"code","metadata":{"id":"S4j1Iupaj9QL","executionInfo":{"status":"ok","timestamp":1620522235582,"user_tz":-540,"elapsed":599,"user":{"displayName":"Akihiro Roppongi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghn5ogbwFTFNltWRIA-1PDOiPxtOW_89Kxmms08iWk=s64","userId":"12484382358284301373"}}},"source":["# logの中身が0になるのを防ぐ\n","def np_log(x):\n","    return np.log(np.clip(x, 1e-10, x))"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QKUyQB8Gp1Pf","executionInfo":{"status":"ok","timestamp":1620522719704,"user_tz":-540,"elapsed":3511,"user":{"displayName":"Akihiro Roppongi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghn5ogbwFTFNltWRIA-1PDOiPxtOW_89Kxmms08iWk=s64","userId":"12484382358284301373"}},"outputId":"e49fbc4b-a38f-4f25-c1d1-64f1b394b356"},"source":["!pip install python-util"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Collecting python-util\n","  Downloading https://files.pythonhosted.org/packages/c8/b3/b2a65ab7c591139193bb4f1fa2c870b0d833a3b28ad2c65a1b3e505f2d17/python_util-1.2.1-py3-none-any.whl\n","Installing collected packages: python-util\n","Successfully installed python-util-1.2.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pP9pHH6NsZ17","executionInfo":{"status":"ok","timestamp":1620522940726,"user_tz":-540,"elapsed":3757,"user":{"displayName":"Akihiro Roppongi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghn5ogbwFTFNltWRIA-1PDOiPxtOW_89Kxmms08iWk=s64","userId":"12484382358284301373"}},"outputId":"289dddc2-2253-4bcf-dab8-12db09a07aef"},"source":["!pip install node-utils"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Collecting node-utils\n","  Downloading https://files.pythonhosted.org/packages/b2/0d/2c501ae3da89e7d672acabc1f41ba8efd59d170a7343320f982a0964e8cf/node_utils-0.2.1-py3-none-any.whl\n","Collecting dataclassy<0.8.0,>=0.7.2\n","  Downloading https://files.pythonhosted.org/packages/65/6d/dc7f1537888fe6ef51cd0f83722d2c1610cf81d3ac9d5d3d948566f82913/dataclassy-0.7.2-py3-none-any.whl\n","Installing collected packages: dataclassy, node-utils\n","Successfully installed dataclassy-0.7.2 node-utils-0.2.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ytRW599aj9QM","executionInfo":{"status":"ok","timestamp":1620523282945,"user_tz":-540,"elapsed":621,"user":{"displayName":"Akihiro Roppongi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghn5ogbwFTFNltWRIA-1PDOiPxtOW_89Kxmms08iWk=s64","userId":"12484382358284301373"}}},"source":["import copy\n","import node\n","#from utils import Node\n","\n","def beam_search(input_image, bos_eos, K=5, max_output_length = 100):\n","    u_map, *states = encoder_model.predict(input_image)\n","    \n","    # [score, y_pred, attention, states_tm1]\n","    candidates = [[0, np.array(bos_eos[0]), np.empty((0, 7*7)), states]]\n","    root = node(bos_eos[0][0])\n","        \n","    t = 0\n","    while t < max_output_length:\n","        t += 1\n","        \n","        # すべての候補を一時的に保管するリスト\n","        tmp_candidates = []\n","        \n","        # </s>がすべての候補で出力されたかどうかのフラッグ\n","        end_flag = True\n","        for score_sum, y_pred, attention_seq, states_prev in candidates:\n","            attention_seq = copy.deepcopy(attention_seq)\n","            if y_pred[-1] == bos_eos[1]:\n","                tmp_candidates.append([score_sum, y_pred, attention_seq, states_prev])\n","            else:\n","                end_flag = False\n","                decoded_seq, *states = decoder_model.predict([y_pred[-1:]] + states_prev)\n","                token_dist, attention = attention_model.predict([u_map, decoded_seq])\n","                \n","                # 確率の高い単語（＝対数尤度の高い単語）とそのidを取得（上位K個）\n","                y_loglikelihood = np_log(token_dist.flatten())\n","                y_t, s_t = np.argsort(y_loglikelihood)[::-1][:K], np.sort(y_loglikelihood)[::-1][:K] # argsortは昇順なので反転\n","                \n","                # スコア (対数尤度) を蓄積（s_core_tm1はスカラー、s_tはベクトル）\n","                s_t = s_t + score_sum\n","                \n","                # すべての候補を一時的に保管\n","                tmp_candidates.extend(\n","                    [[s_tk, np.append(y_pred, [y_tk]), np.append(attention_seq, [attention.flatten()], axis=0), states] for s_tk, y_tk in zip(s_t, y_t)]\n","                )\n","        if end_flag:\n","            break\n","        \n","        # 正規化したスコアでソートし、上位K個の候補を保存 ()\n","        candidates = sorted(tmp_candidates, key=lambda x: x[0]/len(x[1]), reverse=True)[:K]\n","        \n","        # Beam Search可視化用\n","        root.depth += 1\n","        for _, y_pred, _, _ in candidates:\n","            root.add_child(y_pred)\n","    \n","    # 最もスコアの高い候補を返す\n","    return candidates[0][1], root, candidates[0][2]"],"execution_count":35,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"54suixh7j9QQ"},"source":["参考までに、通常の貪欲decoding関数の関数は次のようになります。"]},{"cell_type":"code","metadata":{"id":"Pb9hBs8gj9QR","executionInfo":{"status":"ok","timestamp":1620523284807,"user_tz":-540,"elapsed":823,"user":{"displayName":"Akihiro Roppongi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghn5ogbwFTFNltWRIA-1PDOiPxtOW_89Kxmms08iWk=s64","userId":"12484382358284301373"}}},"source":["def decode_sequence(input_image, bos_eos, max_output_length = 100):\n","    u_map, *states = encoder_model.predict(input_image)\n","\n","    target_seq = np.array(bos_eos[0])  # bos_eos[0]=\"<s>\"に対応するインデックス\n","    output_seq = bos_eos[0]\n","    attention_seq = np.empty((0, 7*7))\n","    \n","    while True:\n","        decoded_seq, *states = decoder_model.predict([target_seq] + states)\n","        token_dist, attention = attention_model.predict([u_map, decoded_seq])\n","        sampled_token_index = [np.argmax(token_dist[0, -1, :])]\n","        output_seq += sampled_token_index\n","        attention_seq = np.append(attention_seq, [attention.flatten()], axis=0)\n","        \n","        if (sampled_token_index == bos_eos[1] or len(output_seq) > max_output_length):\n","            break\n","\n","        target_seq = np.array(sampled_token_index)\n","\n","    return output_seq, attention_seq"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"lNzG5vqej9QS"},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","test_no = 100\n","\n","bos_eos = tokenizer_train.texts_to_sequences([\"<s>\", \"</s>\"])\n","detokenizer_train = dict(map(reversed, tokenizer_train.word_index.items()))\n","\n","# x_test = np.array(x_valid[test_no:test_no+1])\n","x_test = np.array(x_train[test_no:test_no+1])\n","\n","y_pred, root, att_a= beam_search(x_test, bos_eos, K=3, max_output_length=25)\n","#y_pred, att_a= decode_sequence(x_test, bos_eos, max_output_length=25)\n","\n","detokenizer_train[0] = \"\"\n","print(' '.join([detokenizer_train[i] for i in y_pred]))\n","\n","plt.imshow(x_test[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mUDdFrrFj9QU"},"source":["ここではさらに、Attentionの分布も描画してみましょう。"]},{"cell_type":"code","metadata":{"id":"cZwQ3ZOuj9QV"},"source":["import skimage.transform\n","\n","fig = plt.figure(figsize=(18, 10))\n","\n","for i, (wordid, att_a_t) in enumerate(zip(y_pred[1:], att_a)):\n","    ax = fig.add_subplot(5, 5, i+1)\n","    \n","    # Plot image\n","    ax.imshow(x_test[0])\n","    ax.axis('off')\n","    \n","    # Plot attention\n","    att_a_t = skimage.transform.pyramid_expand(att_a_t.reshape(7, 7), upscale=32, sigma=20, multichannel=False)\n","    ax.imshow(att_a_t, alpha=.65)\n","    # Plot word\n","    ax.set_title(detokenizer_train[wordid])\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"midmTZzej9QW"},"source":["また次のようにbeam searchの過程を可視化できます。"]},{"cell_type":"code","metadata":{"id":"YwqAXni9j9QX"},"source":["fig = plt.figure(figsize=(30, 10))\n","\n","ax = fig.add_subplot(111, xticks=[], yticks=[])\n","ax.axis('off')\n","\n","root.mark_best_path(y_pred)\n","\n","root.set_coordinates(0, 0, ax, detokenizer_train, tree_depth=root.depth)\n","\n","plt.show()"],"execution_count":null,"outputs":[]}]}