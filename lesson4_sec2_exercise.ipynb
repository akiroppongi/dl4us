{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"lesson4_sec2_exercise.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"lNJvY2BFKsa8"},"source":["# Lesson4 ニューラル翻訳モデルを作ってみよう（Seq2Seq, Attention）"]},{"cell_type":"markdown","metadata":{"id":"xUymj4GIKsbD"},"source":["## 目次\n","\n","- Section1 Checkクイズの解答\n","- Section2 実装①\n","    - 2.0 データの用意\n","    - 2.1 モデル構築\n","    - 2.2 モデルの学習\n","    - 2.3 モデルによる予測\n","    - 2.4 モデルの可視化\n","    - 2.5 機械翻訳の評価について（補足）"]},{"cell_type":"markdown","metadata":{"id":"riTIyQJfKsbE"},"source":["## Section1 Checkクイズの解答\n","\n","問題1: 3, 問題2: 1, 問題3; 2, 問題4: 3"]},{"cell_type":"markdown","metadata":{"id":"IeYeShP7KsbE"},"source":["## Section2 実装①"]},{"cell_type":"markdown","metadata":{"id":"XXtLFCzoKsbE"},"source":["LSTMを使ったSeq2Seqモデルで英日機械翻訳を行ってみましょう。\n","\n","使用するデータセット、train.enとtrain.jaの中身は次のようになっています.\n","\n","train.enの中身 (英語の文)\n","```\n","i can 't tell who will arrive first .\n","many animals have been destroyed by men .\n","i 'm in the tennis club .\n","︙\n","```\n","\n","train.jaの中身(日本語の文, 対訳)\n","```\n","誰 が 一番 に 着 く か 私 に は 分か り ま せ ん 。\n","多く の 動物 が 人間 に よ っ て 滅ぼ さ れ た 。\n","私 は テニス 部員 で す 。\n","︙\n","```\n","(データセットにはTanaka Corpus ( http://www.edrdg.org/wiki/index.php/Tanaka_Corpus )の一部を抽出した \n","small_parallel_enja: 50k En/Ja Parallel Corpus for Testing SMT Methods ( https://github.com/odashi/small_parallel_enja ) を使っています.)"]},{"cell_type":"markdown","metadata":{"id":"B5r3wpQVKsbF"},"source":["### 2.0 データの用意\n","\n","まずはデータの読み込みです。\n","\n","読み込む際、文頭を表す仮想単語（**BOS**, Beginning Of Sentence）として`<s>`、文末を表す仮想単語（**EOS**, End Of Sentence）として`<\\s>`を付加します。\n","\n","また、BOS, EOSをつけた文章について、Tokenizerによって数値化を行います。\n","\n","最後に、バッチ処理のため、各系列の長さをそろえておきます。これには`keras.preprocessing.sequence.pad_sequences`を用います。\n","\n","詳しくは、https://keras.io/ja/preprocessing/sequence/#pad_sequences を参照してください。\n","\n","<img src='figures/preprocess.png' width='80%'>"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y2gqE11ALLZf","executionInfo":{"status":"ok","timestamp":1620480687040,"user_tz":-540,"elapsed":16718,"user":{"displayName":"Akihiro Roppongi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghn5ogbwFTFNltWRIA-1PDOiPxtOW_89Kxmms08iWk=s64","userId":"12484382358284301373"}},"outputId":"bf6e733c-41c3-4a4f-d192-2a18e0f1777f"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eIyGuXnFKsbF"},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","def load_data(file_path):\n","    tokenizer = Tokenizer(filters=\"\")\n","    whole_texts = []\n","    for line in open(file_path, encoding='utf-8'):\n","        whole_texts.append(\"<s> \" + line.strip() + \" </s>\")\n","        \n","    tokenizer.fit_on_texts(whole_texts)\n","    \n","    return tokenizer.texts_to_sequences(whole_texts), tokenizer\n","\n","# 読み込み＆Tokenizerによる数値化\n","x_train, tokenizer_en = load_data('/content/gdrive/MyDrive/dl4us/dl4us-master/lesson4/data/train.en')\n","y_train, tokenizer_ja = load_data('/content/gdrive/MyDrive/dl4us/dl4us-master/lesson4/data/train.ja')\n","\n","en_vocab_size = len(tokenizer_en.word_index) + 1\n","ja_vocab_size = len(tokenizer_ja.word_index) + 1\n","\n","x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.02, random_state=42)\n","\n","# パディング\n","x_train = pad_sequences(x_train, padding='post')\n","y_train = pad_sequences(y_train, padding='post')\n","\n","seqX_len = len(x_train[0])\n","seqY_len = len(y_train[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oIKqmlDsKsbG"},"source":["### 2.1 モデル構築\n","\n","ここでは、LSTMを使用してSeq2Seqモデルを構築します。\n","\n","Embeddingレイヤーでは`mask_zero=True`を引数として指定することで、計算上先程のパディング部分を無視するようにしています。\n","\n","また、Recurrentレイヤーに対する`return_state=True`や`return_sequences=True`の指定をLSTMレイヤーの生成時に行っています。\n","\n","なお、Functional APIによるモデル構築であることに注意してください。\n","\n","<img src='figures/model.png'>\n","\n","なお、図の各レイヤーは以下のように対応することに注意してください。\n","\n","1. 符号化器Embeddingレイヤー： EncoderのEmbedding\n","2. 符号化器再帰レイヤー：　Encoder(LSTM)\n","3. 復号化器Embeddingレイヤー： DecoderのEmbedding\n","4. 復号化器再帰レイヤー： Decoder(LSTM)\n","5. 復号化器出力レイヤー： Dense"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"vy9x-RhBKsbG"},"source":["from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Embedding, Dense, LSTM\n","\n","emb_dim = 256\n","hid_dim = 256\n","\n","## 符号化器\n","# Inputレイヤー（返り値としてテンソルを受け取る）\n","encoder_inputs = Input(shape=(seqX_len,))\n","\n","# モデルの層構成（手前の層の返り値テンソルを、次の接続したい層に別途引数として与える）\n","# InputレイヤーとEmbeddingレイヤーを接続（+Embeddingレイヤーのインスタンス化）\n","encoder_embedded = Embedding(en_vocab_size, emb_dim, mask_zero=True)(encoder_inputs) # shape: (seqX_len,)->(seqX_len, emb_dim)\n","# EmbeddingレイヤーとLSTMレイヤーを接続（+LSTMレイヤーのインスタンス化）\n","_, *encoder_states = LSTM(hid_dim, return_state=True)(encoder_embedded)  # shape: (seqX_len, emb_dim)->(hid_dim, )\n","# このLSTMレイヤーの出力に関しては下記に補足あり"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"m73NijvnKsbH"},"source":["## 復号化器\n","# Inputレイヤー（返り値としてテンソルを受け取る）\n","decoder_inputs = Input(shape=(seqY_len,))\n","\n","# モデルの層構成（手前の層の返り値テンソルを、次の接続したい層に別途引数として与える）\n","# InputレイヤーとEmbeddingレイヤーを接続\n","decoder_embedding = Embedding(ja_vocab_size, emb_dim) # 後で参照したいので、レイヤー自体を変数化\n","decoder_embedded = decoder_embedding(decoder_inputs)  # shape: (seqY_len,)->(seqY_len, emb_dim)\n","# EmbeddingレイヤーとLSTMレイヤーを接続（encoder_statesを初期状態として指定）\n","decoder_lstm = LSTM(hid_dim, return_sequences=True, return_state=True) # 後で参照したいので、レイヤー自体を変数化\n","decoder_outputs, _, _ = decoder_lstm(decoder_embedded, initial_state=encoder_states) # shape: (seqY_len, emb_dim)->(seqY_len, hid_dim)\n","# LSTMレイヤーとDenseレイヤーを接続\n","decoder_dense = Dense(ja_vocab_size, activation='softmax') # 後で参照したいので、レイヤー自体を変数化\n","decoder_outputs = decoder_dense(decoder_outputs) # shape: (seqY_len, hid_dim)->(seqY_len, ja_vocab_size)\n","\n","# モデル構築（入力は符号化器＆復号化器、出力は復号化器のみ）\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n","# 今回は、sparse_categorical_crossentropy（正解ラベルとしてone_hot表現のベクトルでなく数値を受け取るcategorical_crossentropy）を使用"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ztS6j7ZrKsbH"},"source":["#### LSTMレイヤーの補足\n","\n","LSTMの出力に関して補足の説明をします。以下の図はLSTMの模式図です。\n","\n","<img src='../figures/lstm.png' width='50%'>\n","\n","**1. LSTMレイヤーはオプションがない場合はレイヤーの出力を返します。**\n","\n","```\n","output = LSTM()(x)\n","```\n","\n","なお、このときの出力(output)とはLSTMの最後の隠れ状態$h$のことを指します。\n","\n","図を3timestepとみる場合は $h_{t+2}$にあたります。\n","\n","**2.  引数に`return_state=True`を指定されているときは隠れ状態も返します。**\n","\n","```\n","output, state_h, state_c = LSTM(return_state=True)(x)\n","```\n","\n","このときの`state_h`と`state_c`はそれぞれLSTMの最後の隠れ状態$h$とセル状態$c$となります。\n","\n","図を3timestepとみる場合は `state_h`は $h_{t+2}$ 、`state_c`は$c_{t+2}$にあたります。\n","\n","このとき、`output=state_h`となっていることに注意してください。\n","\n","また、以下のコードを説明すると、\n","\n","```\n","_, *encoder_states = LSTM(hid_dim, return_state=True)(encoder_embedded)\n","```\n","\n","outputは今回は参照しないので`_`(アンダーバー)を使っており、`*`(スター)を使うことで`encoder_states=[state_h, state_c]`となるような代入を行っています。このような使い方はpython一般の使い方ですので疑問に思った方はpythonの復習をおすすめします。\n","\n","**3.  引数に`return_sequences=True`を指定されているときは系列も返します。**\n","\n","```\n","outputs, state_h, state_c = LSTM(return_state=True, return_sequences=True)(x)\n","```\n","\n","このとき`outputs`は系列すべての出力を含みます。\n","\n","図を3timestepとみる場合は、`outputs`に $h_{t}$、$h_{t+1}$、$h_{t+2}$のすべての出力を含むということになります。"]},{"cell_type":"markdown","metadata":{"id":"hwPv3xBxKsbI"},"source":["### 2.2 モデルの学習\n","\n","モデルの学習時には、教師データとして1時点先の単語を示すデータを入力します。(`train_target`)\n","\n","学習時にはDecoderの入力に教師データを用います。\n","\n","<img src='figures/training.png'>"]},{"cell_type":"code","metadata":{"collapsed":true,"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"oxUJva32KsbI","executionInfo":{"status":"ok","timestamp":1620483661074,"user_tz":-540,"elapsed":2222769,"user":{"displayName":"Akihiro Roppongi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghn5ogbwFTFNltWRIA-1PDOiPxtOW_89Kxmms08iWk=s64","userId":"12484382358284301373"}},"outputId":"7cc16a1f-fd30-44db-a205-8a623f08dee5"},"source":["import numpy as np\n","\n","train_target = np.hstack((y_train[:, 1:], np.zeros((len(y_train),1), dtype=np.int32)))\n","\n","model.fit([x_train, y_train], np.expand_dims(train_target, -1), batch_size=64, epochs=5, verbose=2, validation_split=0.2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","613/613 - 446s - loss: 2.6017 - val_loss: 2.0712\n","Epoch 2/5\n","613/613 - 446s - loss: 1.9011 - val_loss: 1.7967\n","Epoch 3/5\n","613/613 - 446s - loss: 1.6806 - val_loss: 1.6476\n","Epoch 4/5\n","613/613 - 441s - loss: 1.5287 - val_loss: 1.5380\n","Epoch 5/5\n","613/613 - 442s - loss: 1.4086 - val_loss: 1.4526\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f38f48d0c50>"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"5gVH8pqvKsbJ"},"source":["### 2.3 モデルによる生成\n","\n","先程学習したモデルを使用して、系列を生成してみましょう。\n","\n","そのためにまずは学習したモデルを組み込んだ、系列生成用のモデルを構築します。\n","\n","学習時との違いは、復号化器が1ステップずつ実行できるよう、状態ベクトルの入力と出力をモデルの定義に加えている点です。\n","\n","(また、1ステップ前の状態を引き継いで生成が可能になるように、復号化器のモデルの初期状態を指定可能にしています。)\n","\n","生成する際のDecoderの入力には翻訳先の教師データは用いません。\n","\n","<img src='figures/prediction.png'>"]},{"cell_type":"code","metadata":{"id":"xxQWFtqAKsbJ"},"source":["# サンプリング用（生成用）のモデルを作成\n","\n","# 符号化器（学習時と同じ構成、学習したレイヤーを利用）\n","encoder_model = Model(encoder_inputs, encoder_states)\n","\n","# 復号化器\n","decoder_states_inputs = [Input(shape=(hid_dim,)), Input(shape=(hid_dim,))] # decorder_lstmの初期状態指定用(h_t, c_t)\n","\n","decoder_inputs = Input(shape=(1,))\n","decoder_embedded = decoder_embedding(decoder_inputs) # 学習済みEmbeddingレイヤーを利用\n","decoder_outputs, *decoder_states = decoder_lstm(decoder_embedded, initial_state=decoder_states_inputs) # 学習済みLSTMレイヤーを利用\n","decoder_outputs = decoder_dense(decoder_outputs) # 学習済みDenseレイヤーを利用\n","\n","decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ucNiVaqyKsbJ"},"source":["このモデルを使用した生成（予測）を行ってみましょう。\n","\n","生成では、未知のデータに対してモデルを適用するので正解ラベルはわかりません。\n","\n","そこで、代わりに前のステップで予測した単語を各ステップでの入力とします。\n","\n","そして, 系列の終わりを表す単語 (`</s>`) が出力されるまで繰り返します。（最初の入力は`<s>`を使用します）"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"un-zPxgUKsbJ"},"source":["def decode_sequence(input_seq, bos_eos, max_output_length = 1000):\n","    states_value = encoder_model.predict(input_seq)\n","\n","    target_seq = np.array(bos_eos[0])  # bos_eos[0]=\"<s>\"に対応するインデックス\n","    output_seq= bos_eos[0][:]\n","    \n","    while True:\n","        output_tokens, *states_value = decoder_model.predict([target_seq] + states_value)\n","        sampled_token_index = [np.argmax(output_tokens[0, -1, :])]\n","        output_seq += sampled_token_index\n","        \n","        if (sampled_token_index == bos_eos[1] or len(output_seq) > max_output_length):\n","            break\n","\n","        target_seq = np.array(sampled_token_index)\n","\n","    return output_seq"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"VNdQtse0KsbK","executionInfo":{"status":"ok","timestamp":1620484792949,"user_tz":-540,"elapsed":2960,"user":{"displayName":"Akihiro Roppongi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghn5ogbwFTFNltWRIA-1PDOiPxtOW_89Kxmms08iWk=s64","userId":"12484382358284301373"}},"outputId":"5e90131c-29c9-4837-f98e-b66911db3d13"},"source":["detokenizer_en = dict(map(reversed, tokenizer_en.word_index.items()))\n","detokenizer_ja = dict(map(reversed, tokenizer_ja.word_index.items()))\n","\n","text_no = 0\n","input_seq = pad_sequences([x_test[text_no]], seqX_len, padding='post')\n","bos_eos = tokenizer_ja.texts_to_sequences([\"<s>\", \"</s>\"])\n","\n","print('元の文:', ' '.join([detokenizer_en[i] for i in x_test[text_no]]))\n","print('生成文:', ' '.join([detokenizer_ja[i] for i in decode_sequence(input_seq, bos_eos)]))\n","print('正解文:', ' '.join([detokenizer_ja[i] for i in y_test[text_no]]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["元の文: <s> you may extend your stay in tokyo . </s>\n","生成文: <s> あなた は 東京 に 住 ん で い る よ 。 </s>\n","正解文: <s> 東京 滞在 を 延ば し て も い い で す よ 。 </s>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EeEMjbQcKsbK"},"source":["### 2.4 モデルの可視化"]},{"cell_type":"code","metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/","height":620},"id":"pOzhhUXjKsbK","executionInfo":{"status":"ok","timestamp":1620484796128,"user_tz":-540,"elapsed":859,"user":{"displayName":"Akihiro Roppongi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghn5ogbwFTFNltWRIA-1PDOiPxtOW_89Kxmms08iWk=s64","userId":"12484382358284301373"}},"outputId":"08e5e619-b88e-4081-8a00-de7293b0345b"},"source":["from IPython.display import SVG\n","from tensorflow.python.keras.utils.vis_utils import model_to_dot\n","\n","SVG(model_to_dot(model).create(prog='dot', format='svg'))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<IPython.core.display.SVG object>"],"image/svg+xml":"<svg height=\"449pt\" viewBox=\"0.00 0.00 345.00 337.00\" width=\"460pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1.3333 1.3333) rotate(0) translate(4 333)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-333 341,-333 341,4 -4,4\" stroke=\"transparent\"/>\n<!-- 139882456627408 -->\n<g class=\"node\" id=\"node1\">\n<title>139882456627408</title>\n<polygon fill=\"none\" points=\"19,-292.5 19,-328.5 152,-328.5 152,-292.5 19,-292.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.5\" y=\"-306.8\">input_3: InputLayer</text>\n</g>\n<!-- 139882456627344 -->\n<g class=\"node\" id=\"node3\">\n<title>139882456627344</title>\n<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 171,-255.5 171,-219.5 0,-219.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.5\" y=\"-233.8\">embedding_2: Embedding</text>\n</g>\n<!-- 139882456627408&#45;&gt;139882456627344 -->\n<g class=\"edge\" id=\"edge1\">\n<title>139882456627408-&gt;139882456627344</title>\n<path d=\"M85.5,-292.4551C85.5,-284.3828 85.5,-274.6764 85.5,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"89.0001,-265.5903 85.5,-255.5904 82.0001,-265.5904 89.0001,-265.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139882577441680 -->\n<g class=\"node\" id=\"node2\">\n<title>139882577441680</title>\n<polygon fill=\"none\" points=\"189,-219.5 189,-255.5 322,-255.5 322,-219.5 189,-219.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"255.5\" y=\"-233.8\">input_4: InputLayer</text>\n</g>\n<!-- 139882577442128 -->\n<g class=\"node\" id=\"node4\">\n<title>139882577442128</title>\n<polygon fill=\"none\" points=\"166,-146.5 166,-182.5 337,-182.5 337,-146.5 166,-146.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"251.5\" y=\"-160.8\">embedding_3: Embedding</text>\n</g>\n<!-- 139882577441680&#45;&gt;139882577442128 -->\n<g class=\"edge\" id=\"edge2\">\n<title>139882577441680-&gt;139882577442128</title>\n<path d=\"M254.5112,-219.4551C254.0689,-211.3828 253.5371,-201.6764 253.0442,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"256.5332,-192.3839 252.4913,-182.5904 249.5437,-192.7669 256.5332,-192.3839\" stroke=\"#000000\"/>\n</g>\n<!-- 139882459414416 -->\n<g class=\"node\" id=\"node5\">\n<title>139882459414416</title>\n<polygon fill=\"none\" points=\"41.5,-146.5 41.5,-182.5 143.5,-182.5 143.5,-146.5 41.5,-146.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"92.5\" y=\"-160.8\">lstm_2: LSTM</text>\n</g>\n<!-- 139882456627344&#45;&gt;139882459414416 -->\n<g class=\"edge\" id=\"edge3\">\n<title>139882456627344-&gt;139882459414416</title>\n<path d=\"M87.2303,-219.4551C88.0044,-211.3828 88.9351,-201.6764 89.7976,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"93.2947,-192.8788 90.7653,-182.5904 86.3267,-192.2106 93.2947,-192.8788\" stroke=\"#000000\"/>\n</g>\n<!-- 139882577538832 -->\n<g class=\"node\" id=\"node6\">\n<title>139882577538832</title>\n<polygon fill=\"none\" points=\"118.5,-73.5 118.5,-109.5 220.5,-109.5 220.5,-73.5 118.5,-73.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"169.5\" y=\"-87.8\">lstm_3: LSTM</text>\n</g>\n<!-- 139882577442128&#45;&gt;139882577538832 -->\n<g class=\"edge\" id=\"edge4\">\n<title>139882577442128-&gt;139882577538832</title>\n<path d=\"M231.2303,-146.4551C221.0788,-137.4177 208.6262,-126.3319 197.5709,-116.4899\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"199.617,-113.6255 189.8207,-109.5904 194.9625,-118.8539 199.617,-113.6255\" stroke=\"#000000\"/>\n</g>\n<!-- 139882459414416&#45;&gt;139882577538832 -->\n<g class=\"edge\" id=\"edge5\">\n<title>139882459414416-&gt;139882577538832</title>\n<path d=\"M111.5337,-146.4551C121.0663,-137.4177 132.7595,-126.3319 143.1408,-116.4899\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"145.5693,-119.0104 150.4184,-109.5904 140.7533,-113.9305 145.5693,-119.0104\" stroke=\"#000000\"/>\n</g>\n<!-- 139882577444816 -->\n<g class=\"node\" id=\"node7\">\n<title>139882577444816</title>\n<polygon fill=\"none\" points=\"116,-.5 116,-36.5 223,-36.5 223,-.5 116,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"169.5\" y=\"-14.8\">dense_1: Dense</text>\n</g>\n<!-- 139882577538832&#45;&gt;139882577444816 -->\n<g class=\"edge\" id=\"edge6\">\n<title>139882577538832-&gt;139882577444816</title>\n<path d=\"M169.5,-73.4551C169.5,-65.3828 169.5,-55.6764 169.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"173.0001,-46.5903 169.5,-36.5904 166.0001,-46.5904 173.0001,-46.5903\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"_aLXSWUxKsbL"},"source":["### 2.5 機械翻訳の評価について（補足）\n","\n","これまで、基本的に学習したモデルの良し悪しについては損失をベースに考えてきました。\n","\n","ですが機械翻訳の場合、損失と翻訳の精度が必ずしも一致しません。\n","\n","というのも、翻訳においては、単純に各単語が一致しているか否か以上に、意味的な繋がりや表現の流暢さが重要となるためです。\n","\n","また、必ずしも語順についても一致している必要はありません。\n","\n","そこで、そうした翻訳タスク特有の性質を反映した評価指標が必要となります。その代表例として、**BLEUスコア**が挙げられます。\n","\n","BLEUスコアは、n-gram（連続n単語. 主にn=4）がどれだけ生成文と正解文で共有されているかなどを考慮した指標となっています。\n","\n","機械翻訳は本講座の主目的ではないので、ここではその詳細な算出方法等は触れませんが、興味がある方はスクリプト（http://www.nltk.org/_modules/nltk/translate/bleu_score.html ）をのぞいてみてください。"]},{"cell_type":"code","metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"SeTYcPfOKsbL","executionInfo":{"status":"ok","timestamp":1620484800003,"user_tz":-540,"elapsed":1321,"user":{"displayName":"Akihiro Roppongi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghn5ogbwFTFNltWRIA-1PDOiPxtOW_89Kxmms08iWk=s64","userId":"12484382358284301373"}},"outputId":"580e3f9a-4169-4dea-83f8-74f4c30d2edc"},"source":["from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","\n","prediction = ['I', 'am', 'a', 'graduate', 'student', 'at', 'a', 'university']\n","reference = [['I', 'am', 'a', 'graduate', 'student', 'at', 'the', 'university', 'of', 'tokyo']]\n","\n","print(sentence_bleu(reference, prediction))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.5506953149031837\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"K_-WG5d1KsbL","executionInfo":{"status":"ok","timestamp":1620484800787,"user_tz":-540,"elapsed":1083,"user":{"displayName":"Akihiro Roppongi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghn5ogbwFTFNltWRIA-1PDOiPxtOW_89Kxmms08iWk=s64","userId":"12484382358284301373"}},"outputId":"ba2fa366-833e-4eed-90f9-2b6c0ed12291"},"source":["text_no = 1\n","input_seq = pad_sequences([x_test[text_no]], seqX_len, padding='post')\n","bos_eos = tokenizer_ja.texts_to_sequences([\"<s>\", \"</s>\"])\n","\n","prediction = [detokenizer_ja[i] for i in decode_sequence(input_seq, bos_eos)]\n","reference = [[detokenizer_ja[i] for i in y_test[text_no]]]\n","\n","print(prediction)\n","print(reference)\n","\n","print(sentence_bleu(reference, prediction))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['<s>', '私', 'は', '学校', 'に', '行', 'く', '。', '</s>']\n","[['<s>', '私', 'は', '学校', 'で', '勉強', 'する', '。', '</s>']]\n","0.3549481056010053\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jhn158SXKsbL"},"source":["このBLEUスコアの他にも、機械翻訳の評価指標がいくつか提案されています。\n","\n","代表的なものの比較は、\n","\n","N. Graham, \"文レベルの機械翻訳評価尺度に関する調査\", 研究報告自然言語処理, vol. 2013-NL-212, no. 7, pp. 1–8, 2013. (http://phontron.com/paper/neubig13nl212.pdf)\n","\n","などにまとめられているので、機械翻訳に興味のあるかたは参照してみてください。"]}]}